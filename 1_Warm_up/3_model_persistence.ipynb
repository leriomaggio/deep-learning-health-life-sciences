{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One paramount important requirement in DL model training and learning is the ability to store and **save** the internal state of a model for the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to **load and save** a model for:\n",
    "\n",
    "1. **inference**; \n",
    "2. **re-start** the training where we left (i.e. _checkpoint_ )\n",
    "3. **save** the best hyper-parameter configuration in a randomised _grid search_ optimisation\n",
    "4. $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **two** approaches for saving and loading models for inference in PyTorch. \n",
    "\n",
    "The **first** is saving and loading the `state_dict`, and the second is saving and loading the **entire model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's define our (usual) model and optimiser first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = torch.relu(self.linear1(x))\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the entire model using `pickle`\n",
    "\n",
    "We could use the Python `pickle` module to save and load an entire model.\n",
    "\n",
    "Using this approach yields the most intuitive syntax and involves the least amount of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gu19087/opt/anaconda3/envs/dl-torch/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model_serialisation.pkl', 'wb') as pkf: \n",
    "    pickle.dump(model, pkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight        torch.Size([100, 1000]) 100000\n",
      "linear1.bias          torch.Size([100])   100\n",
      "linear2.weight        torch.Size([10, 100]) 1000\n",
      "linear2.bias          torch.Size([10])    10\n"
     ]
    }
   ],
   "source": [
    "with open('model_serialisation.pkl', 'rb') as pkf:\n",
    "    model_pkl = pickle.load(pkf)\n",
    "    for name_str, param in model_pkl.named_parameters():\n",
    "        print(\"{:21} {:19} {}\".format(name_str, str(param.shape), param.numel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However**, this method is far from being flexible: the serialized data is bound to the specific classes and the exact directory structure used when the model is saved. \n",
    "\n",
    "The reason for this is because pickle does not save the model class itself. \n",
    "Rather, it saves a path to the file containing the class, which is used during load time. \n",
    "\n",
    "**For this reason**, your code can break in various ways when used in other projects or after refactors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing `model|optim.state_dict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the learnable parameters (i.e. weights and biases) of a `torch.nn.Module` model are contained in the model’s parameters (accessed with `model.parameters()`). \n",
    "\n",
    "A `state_dict` is simply a Python dictionary object that maps each layer to its parameter tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight        torch.Size([100, 1000]) 100000\n",
      "linear1.bias          torch.Size([100])   100\n",
      "linear2.weight        torch.Size([10, 100]) 1000\n",
      "linear2.bias          torch.Size([10])    10\n"
     ]
    }
   ],
   "source": [
    "# model (named) parameters\n",
    "for name_str, param in model.named_parameters():\n",
    "    print(\"{:21} {:19} {}\".format(name_str, str(param.shape), param.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[ 0.0129, -0.0277,  0.0117,  ..., -0.0011,  0.0143, -0.0153],\n",
       "           [ 0.0233,  0.0099,  0.0282,  ..., -0.0152, -0.0058,  0.0137],\n",
       "           [-0.0149,  0.0187,  0.0192,  ...,  0.0030, -0.0027, -0.0279],\n",
       "           ...,\n",
       "           [-0.0135,  0.0245,  0.0204,  ..., -0.0174,  0.0021,  0.0034],\n",
       "           [ 0.0231,  0.0282,  0.0300,  ...,  0.0051, -0.0141, -0.0308],\n",
       "           [-0.0289,  0.0309, -0.0010,  ...,  0.0190, -0.0300, -0.0228]],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([ 0.0276,  0.0058,  0.0107, -0.0192, -0.0024,  0.0151, -0.0274,  0.0171,\n",
       "            0.0170,  0.0036, -0.0219,  0.0019,  0.0056, -0.0313,  0.0247, -0.0212,\n",
       "           -0.0061,  0.0081, -0.0244, -0.0102,  0.0062,  0.0146, -0.0283, -0.0042,\n",
       "            0.0272,  0.0168, -0.0242,  0.0041,  0.0200,  0.0264, -0.0223,  0.0292,\n",
       "            0.0232,  0.0261,  0.0186, -0.0127,  0.0209, -0.0078, -0.0054, -0.0058,\n",
       "            0.0274,  0.0140, -0.0233, -0.0233, -0.0227,  0.0311, -0.0284, -0.0142,\n",
       "            0.0028, -0.0015, -0.0302,  0.0288,  0.0050,  0.0204, -0.0251, -0.0151,\n",
       "            0.0116,  0.0073,  0.0199, -0.0227, -0.0121,  0.0102, -0.0036, -0.0301,\n",
       "            0.0017, -0.0155, -0.0290,  0.0002,  0.0296, -0.0193,  0.0074,  0.0119,\n",
       "           -0.0166,  0.0154, -0.0187,  0.0072,  0.0048, -0.0152, -0.0279, -0.0140,\n",
       "            0.0066, -0.0037, -0.0155,  0.0060,  0.0244, -0.0207, -0.0030,  0.0056,\n",
       "            0.0060, -0.0171, -0.0028,  0.0223, -0.0062, -0.0147,  0.0306,  0.0030,\n",
       "            0.0156, -0.0204, -0.0092, -0.0282], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([[ 6.0397e-03, -8.7106e-02, -5.7451e-02,  1.8538e-03, -6.7711e-02,\n",
       "             4.9320e-02,  1.4171e-02,  5.4378e-02,  1.4146e-02, -8.6058e-02,\n",
       "             5.0229e-02,  8.7175e-02, -1.7491e-02, -6.8559e-02,  2.4183e-02,\n",
       "            -4.8714e-02, -3.4075e-02, -7.0591e-02, -2.2242e-02, -8.8331e-02,\n",
       "             5.1855e-02, -3.5466e-03, -9.7914e-02, -7.5831e-02,  2.8753e-02,\n",
       "             6.3270e-02, -6.4052e-02,  5.3039e-02,  6.5276e-02,  4.8598e-02,\n",
       "             5.6762e-02,  5.6528e-02, -5.6083e-02, -4.3567e-02, -4.1739e-02,\n",
       "            -1.9754e-02, -9.7998e-02,  4.5380e-02, -8.1140e-02, -5.4604e-02,\n",
       "            -4.0464e-02, -5.8959e-02,  1.6699e-02, -3.1998e-02,  7.6722e-03,\n",
       "             4.3287e-02,  6.3781e-02,  1.3740e-03,  4.1390e-02,  6.3780e-02,\n",
       "             8.3487e-02, -1.0379e-03,  3.4332e-02,  1.0504e-02, -7.5072e-02,\n",
       "             2.0289e-02,  9.3477e-02,  7.4263e-02, -3.5551e-02,  1.4834e-02,\n",
       "            -4.3010e-02,  7.1876e-02,  3.7470e-02,  4.5498e-02,  1.2225e-02,\n",
       "            -6.7099e-02, -9.2879e-02, -9.2626e-02,  8.3658e-02,  5.8638e-02,\n",
       "             2.6543e-02,  1.2648e-02, -5.2592e-02, -8.4065e-03, -6.0760e-02,\n",
       "            -9.0605e-02, -7.7751e-03, -8.6171e-02, -2.3379e-02,  7.8544e-02,\n",
       "             8.0324e-02, -7.7527e-02,  2.4833e-02, -9.1391e-02,  3.4234e-02,\n",
       "            -8.8944e-02, -1.0545e-02,  1.9541e-02, -2.2006e-02,  7.3074e-02,\n",
       "             8.8172e-02, -4.1541e-02, -8.9734e-02, -4.5536e-02, -1.8579e-03,\n",
       "             8.5630e-02, -4.3191e-02,  1.8475e-02, -8.6686e-02,  1.8877e-02],\n",
       "           [-3.3583e-03, -3.8957e-02, -5.4169e-02,  8.7197e-02,  1.8776e-02,\n",
       "             5.4089e-02,  1.1461e-02,  7.9725e-02,  8.9141e-02, -8.7513e-02,\n",
       "            -8.6997e-03, -7.8238e-02, -7.5282e-02,  2.9287e-02,  7.6636e-02,\n",
       "             4.1366e-02, -8.9879e-02, -1.6514e-02, -5.7966e-02, -5.9631e-02,\n",
       "             7.8763e-02, -8.7289e-02,  5.5093e-02, -2.6882e-02, -5.0371e-02,\n",
       "            -9.5760e-02, -3.5820e-03, -5.2561e-02,  2.4092e-02,  8.9301e-02,\n",
       "            -3.6397e-02, -7.7650e-02,  8.3698e-02, -9.9086e-02,  4.5744e-02,\n",
       "             1.8525e-02,  1.3282e-02,  5.5116e-02,  7.2584e-02, -9.9738e-03,\n",
       "             8.7878e-02,  4.1210e-02,  5.3266e-02,  1.7971e-02, -5.1427e-03,\n",
       "            -6.5150e-02,  7.1435e-02,  7.8157e-02,  5.6057e-02,  7.6694e-03,\n",
       "            -8.7516e-03,  4.3877e-02,  6.4443e-02,  9.1313e-02,  2.7966e-02,\n",
       "            -4.9264e-02, -9.9418e-02,  1.7548e-02,  5.5363e-02,  7.8699e-02,\n",
       "             7.8334e-02,  3.1311e-02, -4.7621e-02,  2.2168e-02,  7.1217e-02,\n",
       "             8.9676e-02, -4.5493e-02, -8.3063e-02, -7.9194e-03,  4.2968e-02,\n",
       "            -1.4330e-02,  5.8753e-02,  2.5522e-02, -5.0606e-02, -4.8589e-03,\n",
       "            -6.3464e-02,  2.1676e-02,  8.7757e-02,  9.0938e-02, -9.2778e-02,\n",
       "             8.0187e-02, -5.4807e-02,  3.9872e-02,  9.3799e-02, -3.7557e-02,\n",
       "             2.0098e-02, -1.4448e-02, -3.3486e-02,  6.9724e-02,  4.5098e-02,\n",
       "             4.7195e-02,  8.3146e-02, -1.5775e-02,  9.5246e-02,  7.9584e-02,\n",
       "            -2.0932e-02, -9.7079e-02, -3.7770e-02, -1.4361e-02, -6.5851e-03],\n",
       "           [-7.3068e-02,  9.6765e-02,  1.3639e-02, -5.5968e-02, -1.2070e-02,\n",
       "            -7.2104e-02,  9.0902e-02,  7.9207e-02,  9.5885e-02, -9.7221e-02,\n",
       "             4.1327e-02, -8.8859e-02, -1.6648e-03,  5.6193e-03, -3.1264e-02,\n",
       "             4.3246e-02,  9.2361e-02,  1.0870e-02, -9.3888e-02,  2.1386e-02,\n",
       "             5.4324e-02, -7.1312e-02,  6.5123e-02, -9.0817e-02,  9.3773e-02,\n",
       "             9.9825e-02,  1.8338e-02, -8.5020e-02,  2.7074e-02,  7.3050e-02,\n",
       "            -1.4532e-02,  8.7660e-03, -9.9246e-02, -6.9973e-02,  2.1765e-02,\n",
       "            -2.4508e-02,  4.5531e-02, -7.9878e-03, -9.4078e-02, -6.5869e-02,\n",
       "             9.2926e-02, -7.0529e-02, -6.6713e-02,  2.0497e-03,  6.7184e-03,\n",
       "             4.3883e-02,  1.0719e-02, -2.5432e-02,  3.4595e-03,  6.4158e-02,\n",
       "            -1.6255e-02, -7.8653e-02,  2.6729e-02, -4.4632e-02, -2.5579e-02,\n",
       "            -3.4998e-02, -6.6887e-02,  1.8426e-02, -1.3198e-02,  2.0053e-02,\n",
       "            -7.5295e-02,  9.5105e-02, -6.2293e-02, -3.6116e-02, -3.6820e-02,\n",
       "             8.5946e-02,  1.0105e-02,  1.3194e-02,  1.9207e-02, -8.4800e-02,\n",
       "             9.5676e-02,  9.1723e-02,  3.5921e-02, -2.6434e-02,  1.3105e-02,\n",
       "             9.9024e-02, -1.9139e-02, -2.7808e-02,  9.3036e-02,  8.4069e-02,\n",
       "            -6.6241e-02, -3.2865e-02,  8.0090e-02, -2.1852e-02,  1.9757e-02,\n",
       "             5.5647e-02,  8.2475e-02, -1.9520e-02, -8.9646e-02, -1.6741e-02,\n",
       "             9.9963e-02, -6.0581e-02,  5.2808e-02, -6.7505e-02, -4.6904e-02,\n",
       "            -8.1296e-02,  5.3573e-02,  9.1083e-02,  1.9106e-02, -2.9080e-02],\n",
       "           [ 7.1465e-02, -4.3288e-05, -6.9280e-02, -6.7076e-02,  6.3996e-02,\n",
       "             3.0872e-02, -7.9766e-02,  9.2255e-02,  7.9854e-02,  5.6929e-02,\n",
       "             2.1776e-02,  7.3513e-02,  2.7571e-02,  2.2885e-02, -5.8250e-02,\n",
       "             8.9086e-02,  3.8966e-02, -3.2023e-02, -5.8905e-02,  8.6405e-02,\n",
       "            -7.4906e-02, -7.2720e-02,  5.7642e-03,  3.1756e-02,  1.5929e-02,\n",
       "             6.1962e-02, -6.3816e-02,  1.0647e-02,  9.8364e-02, -4.1501e-02,\n",
       "             7.0460e-03,  4.4556e-02, -9.5004e-02, -8.3058e-02, -5.3030e-02,\n",
       "            -8.4196e-02,  3.9178e-02,  8.1794e-04, -3.9290e-02,  7.3309e-02,\n",
       "             5.9412e-02,  7.7620e-02,  8.3524e-02, -5.4468e-02, -4.4280e-02,\n",
       "            -6.7692e-02,  1.7777e-02,  9.7212e-02, -9.0789e-02,  7.2118e-02,\n",
       "            -3.2341e-02, -5.5326e-02,  4.0588e-02,  9.9750e-02,  3.8025e-02,\n",
       "            -7.6471e-03, -6.9118e-02, -2.7383e-02,  6.1322e-03,  1.9185e-02,\n",
       "            -2.4591e-03,  5.6286e-03, -7.7470e-02, -8.9915e-02,  7.9303e-02,\n",
       "             4.6957e-02, -4.9349e-03, -2.8890e-02,  7.2679e-02, -4.5378e-02,\n",
       "            -5.2123e-02, -8.9835e-02,  1.1287e-02, -8.9529e-03,  9.0742e-04,\n",
       "             5.5114e-02,  9.3845e-02,  4.8538e-02,  9.1878e-02, -4.0535e-02,\n",
       "             1.0247e-02, -2.7165e-02,  4.6268e-02,  7.9237e-02,  4.3156e-02,\n",
       "             5.9693e-02, -3.3915e-02, -4.4982e-02, -8.7630e-02, -5.0843e-02,\n",
       "            -8.2647e-02, -8.2477e-02,  8.0764e-02,  5.0915e-03,  4.6106e-02,\n",
       "            -1.8766e-02, -3.3182e-02,  8.5344e-02, -1.5128e-03, -8.4288e-02],\n",
       "           [ 3.4387e-03,  9.9405e-02,  8.2109e-04, -3.2104e-02, -1.1983e-02,\n",
       "             8.2339e-02,  7.6370e-02, -9.8138e-02, -2.4796e-02, -7.0797e-02,\n",
       "            -5.9370e-02,  9.2313e-02, -1.6375e-02,  3.3266e-02, -9.6662e-02,\n",
       "            -3.2316e-02, -9.0488e-02, -6.1023e-03, -8.1759e-02, -7.0254e-02,\n",
       "             9.6130e-03,  9.3115e-02,  3.1517e-02,  8.8886e-02, -9.9441e-02,\n",
       "            -3.8234e-02, -9.9071e-02, -5.4025e-02, -5.7463e-02,  8.2620e-02,\n",
       "             3.6974e-02,  7.3258e-02,  2.6846e-02,  6.2566e-02,  2.5136e-02,\n",
       "             8.1305e-02, -9.2818e-02, -5.0314e-02, -6.4061e-02, -5.5626e-02,\n",
       "            -5.0043e-02, -7.7857e-02, -3.9889e-02,  3.4864e-03, -7.4790e-02,\n",
       "             9.3081e-02,  9.8013e-02,  4.1346e-02,  5.9070e-02,  8.6867e-02,\n",
       "             5.3637e-02,  7.2297e-02,  9.8437e-02,  5.3412e-02,  5.6891e-02,\n",
       "             2.6427e-02,  4.3960e-02, -3.8807e-02,  1.9828e-02, -2.2710e-02,\n",
       "            -2.6245e-02,  2.1453e-02,  7.5224e-02,  4.1028e-02, -5.1455e-02,\n",
       "             2.2243e-02,  7.9249e-02, -4.6603e-02, -6.7208e-02, -6.0396e-02,\n",
       "             5.0192e-02, -3.1604e-02,  3.6818e-02,  7.0960e-02,  7.7530e-02,\n",
       "             6.5197e-02,  6.9150e-02, -4.6160e-02,  5.2696e-02, -9.1601e-03,\n",
       "            -9.4035e-02, -8.0848e-02,  1.2252e-02,  7.2659e-02, -2.4505e-02,\n",
       "             7.4934e-02, -7.6167e-02,  2.1669e-02,  9.3008e-02,  4.9432e-02,\n",
       "             9.4449e-02,  6.6678e-02,  4.2753e-02, -1.1910e-02,  9.6679e-02,\n",
       "            -6.8529e-02, -1.0157e-03, -9.2545e-02, -9.4833e-02, -6.0401e-02],\n",
       "           [-2.0554e-02, -4.1283e-02,  7.5202e-02, -5.0581e-02,  7.6867e-02,\n",
       "             1.0000e-01, -9.3872e-02, -6.9777e-02, -5.9833e-02, -6.9427e-02,\n",
       "             7.6663e-02, -8.5208e-02,  2.4230e-02, -1.5796e-02, -1.5415e-02,\n",
       "             6.4671e-02,  9.9584e-02,  3.2597e-02, -2.5461e-02, -5.5413e-03,\n",
       "            -1.3413e-02, -3.1326e-02, -2.9192e-02, -4.6567e-02,  9.8243e-02,\n",
       "            -1.1212e-02, -2.3979e-02,  7.7617e-02, -3.7327e-02, -4.7715e-02,\n",
       "            -4.4871e-02, -3.9263e-02,  8.7015e-02,  5.9443e-03, -1.4883e-02,\n",
       "             1.6572e-02, -2.3925e-02, -3.1680e-02, -5.4073e-02, -9.9868e-02,\n",
       "             3.5647e-02,  9.3804e-02, -5.1556e-02, -6.6315e-02,  1.7316e-02,\n",
       "            -5.9026e-02,  1.2335e-02, -8.9687e-02, -3.8119e-03, -8.3790e-02,\n",
       "            -4.2525e-02, -1.3249e-02,  6.1686e-02, -6.5085e-02, -2.7949e-02,\n",
       "            -8.7100e-02, -2.4767e-02, -1.7463e-02,  9.0935e-02, -9.5649e-02,\n",
       "            -8.2664e-02,  5.0175e-03,  2.1225e-02,  7.2999e-02,  8.7847e-02,\n",
       "             6.7847e-02, -5.7107e-04,  5.6071e-02, -8.4907e-02,  1.7269e-02,\n",
       "            -4.0415e-02, -7.4515e-02,  1.3839e-02,  9.1045e-03, -7.7219e-02,\n",
       "             2.3996e-02,  8.7458e-02,  5.0316e-02, -2.2645e-02, -8.2988e-02,\n",
       "            -9.7689e-03, -9.6182e-02,  2.5362e-02,  9.9611e-02,  4.6788e-02,\n",
       "             1.1238e-02, -8.6436e-02,  7.3232e-02, -6.6566e-02,  8.3938e-02,\n",
       "            -2.4745e-02, -8.5759e-02,  5.8885e-03,  2.4950e-02, -6.8162e-02,\n",
       "            -3.2551e-02,  2.0507e-02,  7.8403e-02, -2.4033e-02,  6.6680e-02],\n",
       "           [ 5.7149e-02, -3.7554e-02, -8.5112e-02,  9.3722e-02,  6.8870e-02,\n",
       "             8.3002e-02, -4.0203e-02,  2.8195e-02,  1.5695e-02,  1.4885e-02,\n",
       "            -5.2610e-02, -5.1160e-02, -1.4406e-03,  8.5899e-02, -9.0598e-02,\n",
       "            -6.8469e-02, -6.5987e-02,  6.2049e-02, -4.0249e-02, -7.2779e-02,\n",
       "             4.2711e-02, -3.8364e-03,  3.8309e-02,  9.1223e-02,  6.4874e-02,\n",
       "            -7.0102e-02,  5.6570e-02, -7.5938e-02, -7.7024e-02,  6.4373e-02,\n",
       "            -3.6254e-02, -7.8834e-02,  5.0244e-02, -6.0321e-02, -7.0094e-02,\n",
       "             5.5626e-02, -6.7325e-02,  3.0025e-02, -7.4662e-02, -6.7068e-03,\n",
       "             3.2788e-02, -9.8470e-02, -9.1810e-02,  3.3483e-02,  9.4110e-02,\n",
       "             2.5885e-02, -3.8974e-02,  8.6909e-02,  8.3467e-02, -7.8696e-02,\n",
       "            -7.7835e-02, -8.6607e-02, -2.8971e-02, -2.6093e-02, -9.8127e-02,\n",
       "             3.8763e-03,  4.6259e-02, -7.1629e-02,  4.8900e-02,  6.9722e-02,\n",
       "            -3.2478e-02,  9.2257e-02,  6.7863e-02, -3.6889e-02, -8.8230e-02,\n",
       "            -7.6977e-03, -1.0429e-03, -2.3290e-04, -7.8065e-02,  5.5609e-02,\n",
       "            -9.5768e-02,  5.1733e-02, -7.7599e-02, -8.3914e-02, -2.3168e-02,\n",
       "             1.3477e-02,  8.0447e-02,  5.4271e-03, -8.6737e-02,  3.6465e-02,\n",
       "            -2.1326e-02,  6.7685e-03,  5.9429e-02,  7.4680e-02,  8.4911e-02,\n",
       "             2.6829e-02,  8.1955e-02,  1.3126e-02,  2.6068e-03, -9.2530e-02,\n",
       "             3.2421e-02,  4.9750e-02, -6.1601e-02, -6.4948e-02,  3.6481e-02,\n",
       "            -5.8526e-02,  3.7691e-02, -1.7092e-02, -3.5618e-02, -6.8862e-02],\n",
       "           [-6.6715e-02, -7.3132e-02, -3.5132e-03,  7.7850e-02,  6.3476e-02,\n",
       "             7.1232e-02, -9.6672e-02, -8.2047e-02,  2.2797e-02,  3.1244e-02,\n",
       "            -5.2365e-02,  3.2612e-02,  3.4499e-02, -4.0493e-02,  5.1946e-02,\n",
       "             9.1978e-02, -8.4371e-02,  8.8180e-02,  9.4906e-02,  8.1407e-02,\n",
       "            -8.8640e-02,  2.8114e-03,  5.3087e-02, -2.2679e-02,  7.3953e-02,\n",
       "            -8.8017e-02, -3.9196e-02, -9.2420e-02, -9.9014e-02, -4.2489e-02,\n",
       "             6.2517e-02, -3.1612e-02,  7.9146e-02, -7.6977e-03,  4.8802e-02,\n",
       "            -8.3909e-02, -6.6282e-02, -1.8300e-02, -2.9578e-02,  5.8294e-02,\n",
       "             6.5964e-02,  3.2812e-03,  3.7626e-02,  9.6420e-02, -4.2926e-02,\n",
       "             5.4093e-02, -7.8043e-02, -9.1384e-02, -2.2752e-02, -2.3332e-02,\n",
       "            -2.6411e-02,  7.2564e-02, -8.4488e-03, -5.8523e-02, -5.0776e-02,\n",
       "             3.2122e-02, -9.7434e-02,  3.3793e-02,  2.0783e-02, -4.5941e-02,\n",
       "            -2.6204e-02, -9.7895e-02, -3.2598e-02,  5.6724e-02, -5.1443e-02,\n",
       "             2.9846e-02,  8.4554e-03, -4.5411e-02, -3.3259e-02, -2.0535e-02,\n",
       "            -4.3371e-02,  5.0148e-02,  4.6049e-02, -1.5824e-02, -7.2475e-02,\n",
       "             4.3322e-02, -3.9835e-02,  8.5319e-02, -2.0438e-02, -2.2051e-02,\n",
       "             1.0703e-02, -6.2848e-02, -3.4325e-02, -9.3368e-02,  9.2366e-02,\n",
       "            -8.3445e-02,  9.6797e-02, -9.0583e-02,  6.4380e-03,  2.7341e-02,\n",
       "             1.3602e-02, -7.7465e-03, -3.7057e-03, -6.5682e-02,  6.6202e-02,\n",
       "             6.0016e-02, -9.2970e-02,  8.4253e-02,  3.0414e-02, -8.5505e-02],\n",
       "           [-9.5348e-02, -7.6113e-02, -8.3606e-02,  1.5938e-02, -2.0131e-02,\n",
       "            -2.0699e-02,  3.4306e-04, -7.2682e-02, -6.7024e-03, -8.0483e-03,\n",
       "             9.7141e-03, -6.4989e-02, -6.9469e-02,  3.8024e-02,  5.0706e-02,\n",
       "             2.0047e-02, -2.3145e-02, -6.8218e-02, -5.4835e-02, -5.6266e-02,\n",
       "            -1.5731e-03,  5.5056e-03, -5.9776e-02, -8.9783e-02,  4.0810e-03,\n",
       "             1.0393e-02, -4.2363e-02,  4.7316e-02, -7.1609e-02, -3.1711e-02,\n",
       "             9.2264e-02, -8.2079e-02,  6.6912e-02,  6.4212e-02,  2.1305e-02,\n",
       "            -5.0689e-02, -1.9879e-02, -6.8521e-02,  5.3721e-02, -6.3786e-02,\n",
       "             7.4574e-02,  3.9781e-02, -1.5271e-02,  1.2718e-02, -3.4332e-02,\n",
       "             2.2917e-02,  9.0192e-02,  5.0204e-03,  9.2888e-02, -2.6175e-02,\n",
       "            -5.0326e-02,  9.9106e-02, -1.3483e-02, -7.1479e-02, -1.1978e-02,\n",
       "             4.7299e-02,  6.4482e-02, -5.9913e-02, -7.9387e-02,  9.2318e-02,\n",
       "             2.1284e-02,  9.9712e-02, -7.3051e-02,  9.0756e-02,  7.7301e-02,\n",
       "             9.8834e-02, -4.6585e-02, -6.4026e-02, -1.8898e-02,  6.0776e-02,\n",
       "            -2.7641e-02,  2.5494e-02,  7.2721e-02, -1.5766e-02,  4.7878e-02,\n",
       "            -4.2163e-02,  8.5962e-02,  7.3850e-02, -2.3346e-02, -2.6012e-02,\n",
       "             7.4358e-03,  9.6486e-02,  9.3332e-02, -6.5746e-02,  2.5491e-02,\n",
       "            -4.3182e-02, -5.5005e-02,  2.0531e-02, -1.5092e-02,  2.5460e-02,\n",
       "            -3.0333e-02, -8.8788e-02,  8.4583e-02, -1.8511e-03, -2.2102e-02,\n",
       "            -9.5434e-02, -1.9180e-02,  8.7092e-03,  5.7534e-02,  3.9332e-02],\n",
       "           [-4.6316e-02, -8.3404e-02, -7.9888e-02,  9.1124e-02,  6.2954e-02,\n",
       "             6.5575e-02, -9.3121e-02,  4.2876e-02,  8.9381e-02, -3.9886e-02,\n",
       "             4.9845e-02, -4.5368e-03,  7.6700e-02, -3.5977e-02,  5.4258e-02,\n",
       "             1.9079e-02,  3.3177e-02,  7.7526e-02,  8.3591e-02, -4.2260e-02,\n",
       "             6.7222e-02, -9.2025e-02,  8.3811e-02, -2.6344e-02,  3.6421e-02,\n",
       "             2.7206e-02, -1.9360e-02,  3.7738e-02, -5.1932e-02,  2.7489e-02,\n",
       "            -5.4084e-02, -5.5596e-02, -3.9697e-02, -3.2667e-02, -9.1111e-02,\n",
       "            -7.7667e-02,  5.7508e-02, -2.9396e-02,  7.9757e-02, -3.3773e-03,\n",
       "            -8.4354e-02, -9.1824e-02, -1.5001e-02, -3.5265e-03,  3.3553e-02,\n",
       "            -7.8544e-02,  7.0951e-02,  8.9415e-02, -6.8650e-02, -6.4837e-03,\n",
       "             4.1214e-02,  5.4215e-02,  1.6607e-02, -5.5795e-02, -2.3485e-02,\n",
       "            -5.2561e-02, -3.0251e-02,  8.7573e-02, -8.5913e-02, -5.2281e-02,\n",
       "            -3.9188e-02, -1.4096e-02,  6.3002e-02, -3.8720e-02,  6.5381e-02,\n",
       "            -1.7108e-04, -6.0910e-02,  8.9747e-02, -3.4480e-03,  3.5674e-02,\n",
       "            -9.3960e-02,  4.7458e-02, -9.9309e-02, -7.5367e-02,  6.8660e-02,\n",
       "             1.0661e-02, -1.1495e-02,  5.4564e-02,  2.3786e-03,  3.5486e-02,\n",
       "             7.3573e-02, -8.1635e-02, -6.5729e-02, -7.0058e-02, -2.7226e-02,\n",
       "            -6.3011e-02,  4.7394e-02, -5.4706e-02,  8.1106e-02,  2.3748e-03,\n",
       "            -3.7955e-02, -5.7144e-02, -6.7127e-02, -9.8912e-02,  6.3665e-02,\n",
       "             6.8999e-02,  6.7499e-02, -8.6455e-02,  1.0371e-02,  2.6472e-02]],\n",
       "          requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([-0.0698, -0.0899,  0.0866, -0.0496,  0.0276, -0.0060, -0.0541,  0.0843,\n",
       "            0.0287, -0.0502], requires_grad=True)],\n",
       "  'lr': 0.0001,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = optimizer.param_groups[0]\n",
    "type(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['params', 'lr', 'momentum', 'dampening', 'weight_decay', 'nesterov']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(p.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0129, -0.0277,  0.0117,  ..., -0.0011,  0.0143, -0.0153],\n",
       "         [ 0.0233,  0.0099,  0.0282,  ..., -0.0152, -0.0058,  0.0137],\n",
       "         [-0.0149,  0.0187,  0.0192,  ...,  0.0030, -0.0027, -0.0279],\n",
       "         ...,\n",
       "         [-0.0135,  0.0245,  0.0204,  ..., -0.0174,  0.0021,  0.0034],\n",
       "         [ 0.0231,  0.0282,  0.0300,  ...,  0.0051, -0.0141, -0.0308],\n",
       "         [-0.0289,  0.0309, -0.0010,  ...,  0.0190, -0.0300, -0.0228]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0276,  0.0058,  0.0107, -0.0192, -0.0024,  0.0151, -0.0274,  0.0171,\n",
       "          0.0170,  0.0036, -0.0219,  0.0019,  0.0056, -0.0313,  0.0247, -0.0212,\n",
       "         -0.0061,  0.0081, -0.0244, -0.0102,  0.0062,  0.0146, -0.0283, -0.0042,\n",
       "          0.0272,  0.0168, -0.0242,  0.0041,  0.0200,  0.0264, -0.0223,  0.0292,\n",
       "          0.0232,  0.0261,  0.0186, -0.0127,  0.0209, -0.0078, -0.0054, -0.0058,\n",
       "          0.0274,  0.0140, -0.0233, -0.0233, -0.0227,  0.0311, -0.0284, -0.0142,\n",
       "          0.0028, -0.0015, -0.0302,  0.0288,  0.0050,  0.0204, -0.0251, -0.0151,\n",
       "          0.0116,  0.0073,  0.0199, -0.0227, -0.0121,  0.0102, -0.0036, -0.0301,\n",
       "          0.0017, -0.0155, -0.0290,  0.0002,  0.0296, -0.0193,  0.0074,  0.0119,\n",
       "         -0.0166,  0.0154, -0.0187,  0.0072,  0.0048, -0.0152, -0.0279, -0.0140,\n",
       "          0.0066, -0.0037, -0.0155,  0.0060,  0.0244, -0.0207, -0.0030,  0.0056,\n",
       "          0.0060, -0.0171, -0.0028,  0.0223, -0.0062, -0.0147,  0.0306,  0.0030,\n",
       "          0.0156, -0.0204, -0.0092, -0.0282], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 6.0397e-03, -8.7106e-02, -5.7451e-02,  1.8538e-03, -6.7711e-02,\n",
       "           4.9320e-02,  1.4171e-02,  5.4378e-02,  1.4146e-02, -8.6058e-02,\n",
       "           5.0229e-02,  8.7175e-02, -1.7491e-02, -6.8559e-02,  2.4183e-02,\n",
       "          -4.8714e-02, -3.4075e-02, -7.0591e-02, -2.2242e-02, -8.8331e-02,\n",
       "           5.1855e-02, -3.5466e-03, -9.7914e-02, -7.5831e-02,  2.8753e-02,\n",
       "           6.3270e-02, -6.4052e-02,  5.3039e-02,  6.5276e-02,  4.8598e-02,\n",
       "           5.6762e-02,  5.6528e-02, -5.6083e-02, -4.3567e-02, -4.1739e-02,\n",
       "          -1.9754e-02, -9.7998e-02,  4.5380e-02, -8.1140e-02, -5.4604e-02,\n",
       "          -4.0464e-02, -5.8959e-02,  1.6699e-02, -3.1998e-02,  7.6722e-03,\n",
       "           4.3287e-02,  6.3781e-02,  1.3740e-03,  4.1390e-02,  6.3780e-02,\n",
       "           8.3487e-02, -1.0379e-03,  3.4332e-02,  1.0504e-02, -7.5072e-02,\n",
       "           2.0289e-02,  9.3477e-02,  7.4263e-02, -3.5551e-02,  1.4834e-02,\n",
       "          -4.3010e-02,  7.1876e-02,  3.7470e-02,  4.5498e-02,  1.2225e-02,\n",
       "          -6.7099e-02, -9.2879e-02, -9.2626e-02,  8.3658e-02,  5.8638e-02,\n",
       "           2.6543e-02,  1.2648e-02, -5.2592e-02, -8.4065e-03, -6.0760e-02,\n",
       "          -9.0605e-02, -7.7751e-03, -8.6171e-02, -2.3379e-02,  7.8544e-02,\n",
       "           8.0324e-02, -7.7527e-02,  2.4833e-02, -9.1391e-02,  3.4234e-02,\n",
       "          -8.8944e-02, -1.0545e-02,  1.9541e-02, -2.2006e-02,  7.3074e-02,\n",
       "           8.8172e-02, -4.1541e-02, -8.9734e-02, -4.5536e-02, -1.8579e-03,\n",
       "           8.5630e-02, -4.3191e-02,  1.8475e-02, -8.6686e-02,  1.8877e-02],\n",
       "         [-3.3583e-03, -3.8957e-02, -5.4169e-02,  8.7197e-02,  1.8776e-02,\n",
       "           5.4089e-02,  1.1461e-02,  7.9725e-02,  8.9141e-02, -8.7513e-02,\n",
       "          -8.6997e-03, -7.8238e-02, -7.5282e-02,  2.9287e-02,  7.6636e-02,\n",
       "           4.1366e-02, -8.9879e-02, -1.6514e-02, -5.7966e-02, -5.9631e-02,\n",
       "           7.8763e-02, -8.7289e-02,  5.5093e-02, -2.6882e-02, -5.0371e-02,\n",
       "          -9.5760e-02, -3.5820e-03, -5.2561e-02,  2.4092e-02,  8.9301e-02,\n",
       "          -3.6397e-02, -7.7650e-02,  8.3698e-02, -9.9086e-02,  4.5744e-02,\n",
       "           1.8525e-02,  1.3282e-02,  5.5116e-02,  7.2584e-02, -9.9738e-03,\n",
       "           8.7878e-02,  4.1210e-02,  5.3266e-02,  1.7971e-02, -5.1427e-03,\n",
       "          -6.5150e-02,  7.1435e-02,  7.8157e-02,  5.6057e-02,  7.6694e-03,\n",
       "          -8.7516e-03,  4.3877e-02,  6.4443e-02,  9.1313e-02,  2.7966e-02,\n",
       "          -4.9264e-02, -9.9418e-02,  1.7548e-02,  5.5363e-02,  7.8699e-02,\n",
       "           7.8334e-02,  3.1311e-02, -4.7621e-02,  2.2168e-02,  7.1217e-02,\n",
       "           8.9676e-02, -4.5493e-02, -8.3063e-02, -7.9194e-03,  4.2968e-02,\n",
       "          -1.4330e-02,  5.8753e-02,  2.5522e-02, -5.0606e-02, -4.8589e-03,\n",
       "          -6.3464e-02,  2.1676e-02,  8.7757e-02,  9.0938e-02, -9.2778e-02,\n",
       "           8.0187e-02, -5.4807e-02,  3.9872e-02,  9.3799e-02, -3.7557e-02,\n",
       "           2.0098e-02, -1.4448e-02, -3.3486e-02,  6.9724e-02,  4.5098e-02,\n",
       "           4.7195e-02,  8.3146e-02, -1.5775e-02,  9.5246e-02,  7.9584e-02,\n",
       "          -2.0932e-02, -9.7079e-02, -3.7770e-02, -1.4361e-02, -6.5851e-03],\n",
       "         [-7.3068e-02,  9.6765e-02,  1.3639e-02, -5.5968e-02, -1.2070e-02,\n",
       "          -7.2104e-02,  9.0902e-02,  7.9207e-02,  9.5885e-02, -9.7221e-02,\n",
       "           4.1327e-02, -8.8859e-02, -1.6648e-03,  5.6193e-03, -3.1264e-02,\n",
       "           4.3246e-02,  9.2361e-02,  1.0870e-02, -9.3888e-02,  2.1386e-02,\n",
       "           5.4324e-02, -7.1312e-02,  6.5123e-02, -9.0817e-02,  9.3773e-02,\n",
       "           9.9825e-02,  1.8338e-02, -8.5020e-02,  2.7074e-02,  7.3050e-02,\n",
       "          -1.4532e-02,  8.7660e-03, -9.9246e-02, -6.9973e-02,  2.1765e-02,\n",
       "          -2.4508e-02,  4.5531e-02, -7.9878e-03, -9.4078e-02, -6.5869e-02,\n",
       "           9.2926e-02, -7.0529e-02, -6.6713e-02,  2.0497e-03,  6.7184e-03,\n",
       "           4.3883e-02,  1.0719e-02, -2.5432e-02,  3.4595e-03,  6.4158e-02,\n",
       "          -1.6255e-02, -7.8653e-02,  2.6729e-02, -4.4632e-02, -2.5579e-02,\n",
       "          -3.4998e-02, -6.6887e-02,  1.8426e-02, -1.3198e-02,  2.0053e-02,\n",
       "          -7.5295e-02,  9.5105e-02, -6.2293e-02, -3.6116e-02, -3.6820e-02,\n",
       "           8.5946e-02,  1.0105e-02,  1.3194e-02,  1.9207e-02, -8.4800e-02,\n",
       "           9.5676e-02,  9.1723e-02,  3.5921e-02, -2.6434e-02,  1.3105e-02,\n",
       "           9.9024e-02, -1.9139e-02, -2.7808e-02,  9.3036e-02,  8.4069e-02,\n",
       "          -6.6241e-02, -3.2865e-02,  8.0090e-02, -2.1852e-02,  1.9757e-02,\n",
       "           5.5647e-02,  8.2475e-02, -1.9520e-02, -8.9646e-02, -1.6741e-02,\n",
       "           9.9963e-02, -6.0581e-02,  5.2808e-02, -6.7505e-02, -4.6904e-02,\n",
       "          -8.1296e-02,  5.3573e-02,  9.1083e-02,  1.9106e-02, -2.9080e-02],\n",
       "         [ 7.1465e-02, -4.3288e-05, -6.9280e-02, -6.7076e-02,  6.3996e-02,\n",
       "           3.0872e-02, -7.9766e-02,  9.2255e-02,  7.9854e-02,  5.6929e-02,\n",
       "           2.1776e-02,  7.3513e-02,  2.7571e-02,  2.2885e-02, -5.8250e-02,\n",
       "           8.9086e-02,  3.8966e-02, -3.2023e-02, -5.8905e-02,  8.6405e-02,\n",
       "          -7.4906e-02, -7.2720e-02,  5.7642e-03,  3.1756e-02,  1.5929e-02,\n",
       "           6.1962e-02, -6.3816e-02,  1.0647e-02,  9.8364e-02, -4.1501e-02,\n",
       "           7.0460e-03,  4.4556e-02, -9.5004e-02, -8.3058e-02, -5.3030e-02,\n",
       "          -8.4196e-02,  3.9178e-02,  8.1794e-04, -3.9290e-02,  7.3309e-02,\n",
       "           5.9412e-02,  7.7620e-02,  8.3524e-02, -5.4468e-02, -4.4280e-02,\n",
       "          -6.7692e-02,  1.7777e-02,  9.7212e-02, -9.0789e-02,  7.2118e-02,\n",
       "          -3.2341e-02, -5.5326e-02,  4.0588e-02,  9.9750e-02,  3.8025e-02,\n",
       "          -7.6471e-03, -6.9118e-02, -2.7383e-02,  6.1322e-03,  1.9185e-02,\n",
       "          -2.4591e-03,  5.6286e-03, -7.7470e-02, -8.9915e-02,  7.9303e-02,\n",
       "           4.6957e-02, -4.9349e-03, -2.8890e-02,  7.2679e-02, -4.5378e-02,\n",
       "          -5.2123e-02, -8.9835e-02,  1.1287e-02, -8.9529e-03,  9.0742e-04,\n",
       "           5.5114e-02,  9.3845e-02,  4.8538e-02,  9.1878e-02, -4.0535e-02,\n",
       "           1.0247e-02, -2.7165e-02,  4.6268e-02,  7.9237e-02,  4.3156e-02,\n",
       "           5.9693e-02, -3.3915e-02, -4.4982e-02, -8.7630e-02, -5.0843e-02,\n",
       "          -8.2647e-02, -8.2477e-02,  8.0764e-02,  5.0915e-03,  4.6106e-02,\n",
       "          -1.8766e-02, -3.3182e-02,  8.5344e-02, -1.5128e-03, -8.4288e-02],\n",
       "         [ 3.4387e-03,  9.9405e-02,  8.2109e-04, -3.2104e-02, -1.1983e-02,\n",
       "           8.2339e-02,  7.6370e-02, -9.8138e-02, -2.4796e-02, -7.0797e-02,\n",
       "          -5.9370e-02,  9.2313e-02, -1.6375e-02,  3.3266e-02, -9.6662e-02,\n",
       "          -3.2316e-02, -9.0488e-02, -6.1023e-03, -8.1759e-02, -7.0254e-02,\n",
       "           9.6130e-03,  9.3115e-02,  3.1517e-02,  8.8886e-02, -9.9441e-02,\n",
       "          -3.8234e-02, -9.9071e-02, -5.4025e-02, -5.7463e-02,  8.2620e-02,\n",
       "           3.6974e-02,  7.3258e-02,  2.6846e-02,  6.2566e-02,  2.5136e-02,\n",
       "           8.1305e-02, -9.2818e-02, -5.0314e-02, -6.4061e-02, -5.5626e-02,\n",
       "          -5.0043e-02, -7.7857e-02, -3.9889e-02,  3.4864e-03, -7.4790e-02,\n",
       "           9.3081e-02,  9.8013e-02,  4.1346e-02,  5.9070e-02,  8.6867e-02,\n",
       "           5.3637e-02,  7.2297e-02,  9.8437e-02,  5.3412e-02,  5.6891e-02,\n",
       "           2.6427e-02,  4.3960e-02, -3.8807e-02,  1.9828e-02, -2.2710e-02,\n",
       "          -2.6245e-02,  2.1453e-02,  7.5224e-02,  4.1028e-02, -5.1455e-02,\n",
       "           2.2243e-02,  7.9249e-02, -4.6603e-02, -6.7208e-02, -6.0396e-02,\n",
       "           5.0192e-02, -3.1604e-02,  3.6818e-02,  7.0960e-02,  7.7530e-02,\n",
       "           6.5197e-02,  6.9150e-02, -4.6160e-02,  5.2696e-02, -9.1601e-03,\n",
       "          -9.4035e-02, -8.0848e-02,  1.2252e-02,  7.2659e-02, -2.4505e-02,\n",
       "           7.4934e-02, -7.6167e-02,  2.1669e-02,  9.3008e-02,  4.9432e-02,\n",
       "           9.4449e-02,  6.6678e-02,  4.2753e-02, -1.1910e-02,  9.6679e-02,\n",
       "          -6.8529e-02, -1.0157e-03, -9.2545e-02, -9.4833e-02, -6.0401e-02],\n",
       "         [-2.0554e-02, -4.1283e-02,  7.5202e-02, -5.0581e-02,  7.6867e-02,\n",
       "           1.0000e-01, -9.3872e-02, -6.9777e-02, -5.9833e-02, -6.9427e-02,\n",
       "           7.6663e-02, -8.5208e-02,  2.4230e-02, -1.5796e-02, -1.5415e-02,\n",
       "           6.4671e-02,  9.9584e-02,  3.2597e-02, -2.5461e-02, -5.5413e-03,\n",
       "          -1.3413e-02, -3.1326e-02, -2.9192e-02, -4.6567e-02,  9.8243e-02,\n",
       "          -1.1212e-02, -2.3979e-02,  7.7617e-02, -3.7327e-02, -4.7715e-02,\n",
       "          -4.4871e-02, -3.9263e-02,  8.7015e-02,  5.9443e-03, -1.4883e-02,\n",
       "           1.6572e-02, -2.3925e-02, -3.1680e-02, -5.4073e-02, -9.9868e-02,\n",
       "           3.5647e-02,  9.3804e-02, -5.1556e-02, -6.6315e-02,  1.7316e-02,\n",
       "          -5.9026e-02,  1.2335e-02, -8.9687e-02, -3.8119e-03, -8.3790e-02,\n",
       "          -4.2525e-02, -1.3249e-02,  6.1686e-02, -6.5085e-02, -2.7949e-02,\n",
       "          -8.7100e-02, -2.4767e-02, -1.7463e-02,  9.0935e-02, -9.5649e-02,\n",
       "          -8.2664e-02,  5.0175e-03,  2.1225e-02,  7.2999e-02,  8.7847e-02,\n",
       "           6.7847e-02, -5.7107e-04,  5.6071e-02, -8.4907e-02,  1.7269e-02,\n",
       "          -4.0415e-02, -7.4515e-02,  1.3839e-02,  9.1045e-03, -7.7219e-02,\n",
       "           2.3996e-02,  8.7458e-02,  5.0316e-02, -2.2645e-02, -8.2988e-02,\n",
       "          -9.7689e-03, -9.6182e-02,  2.5362e-02,  9.9611e-02,  4.6788e-02,\n",
       "           1.1238e-02, -8.6436e-02,  7.3232e-02, -6.6566e-02,  8.3938e-02,\n",
       "          -2.4745e-02, -8.5759e-02,  5.8885e-03,  2.4950e-02, -6.8162e-02,\n",
       "          -3.2551e-02,  2.0507e-02,  7.8403e-02, -2.4033e-02,  6.6680e-02],\n",
       "         [ 5.7149e-02, -3.7554e-02, -8.5112e-02,  9.3722e-02,  6.8870e-02,\n",
       "           8.3002e-02, -4.0203e-02,  2.8195e-02,  1.5695e-02,  1.4885e-02,\n",
       "          -5.2610e-02, -5.1160e-02, -1.4406e-03,  8.5899e-02, -9.0598e-02,\n",
       "          -6.8469e-02, -6.5987e-02,  6.2049e-02, -4.0249e-02, -7.2779e-02,\n",
       "           4.2711e-02, -3.8364e-03,  3.8309e-02,  9.1223e-02,  6.4874e-02,\n",
       "          -7.0102e-02,  5.6570e-02, -7.5938e-02, -7.7024e-02,  6.4373e-02,\n",
       "          -3.6254e-02, -7.8834e-02,  5.0244e-02, -6.0321e-02, -7.0094e-02,\n",
       "           5.5626e-02, -6.7325e-02,  3.0025e-02, -7.4662e-02, -6.7068e-03,\n",
       "           3.2788e-02, -9.8470e-02, -9.1810e-02,  3.3483e-02,  9.4110e-02,\n",
       "           2.5885e-02, -3.8974e-02,  8.6909e-02,  8.3467e-02, -7.8696e-02,\n",
       "          -7.7835e-02, -8.6607e-02, -2.8971e-02, -2.6093e-02, -9.8127e-02,\n",
       "           3.8763e-03,  4.6259e-02, -7.1629e-02,  4.8900e-02,  6.9722e-02,\n",
       "          -3.2478e-02,  9.2257e-02,  6.7863e-02, -3.6889e-02, -8.8230e-02,\n",
       "          -7.6977e-03, -1.0429e-03, -2.3290e-04, -7.8065e-02,  5.5609e-02,\n",
       "          -9.5768e-02,  5.1733e-02, -7.7599e-02, -8.3914e-02, -2.3168e-02,\n",
       "           1.3477e-02,  8.0447e-02,  5.4271e-03, -8.6737e-02,  3.6465e-02,\n",
       "          -2.1326e-02,  6.7685e-03,  5.9429e-02,  7.4680e-02,  8.4911e-02,\n",
       "           2.6829e-02,  8.1955e-02,  1.3126e-02,  2.6068e-03, -9.2530e-02,\n",
       "           3.2421e-02,  4.9750e-02, -6.1601e-02, -6.4948e-02,  3.6481e-02,\n",
       "          -5.8526e-02,  3.7691e-02, -1.7092e-02, -3.5618e-02, -6.8862e-02],\n",
       "         [-6.6715e-02, -7.3132e-02, -3.5132e-03,  7.7850e-02,  6.3476e-02,\n",
       "           7.1232e-02, -9.6672e-02, -8.2047e-02,  2.2797e-02,  3.1244e-02,\n",
       "          -5.2365e-02,  3.2612e-02,  3.4499e-02, -4.0493e-02,  5.1946e-02,\n",
       "           9.1978e-02, -8.4371e-02,  8.8180e-02,  9.4906e-02,  8.1407e-02,\n",
       "          -8.8640e-02,  2.8114e-03,  5.3087e-02, -2.2679e-02,  7.3953e-02,\n",
       "          -8.8017e-02, -3.9196e-02, -9.2420e-02, -9.9014e-02, -4.2489e-02,\n",
       "           6.2517e-02, -3.1612e-02,  7.9146e-02, -7.6977e-03,  4.8802e-02,\n",
       "          -8.3909e-02, -6.6282e-02, -1.8300e-02, -2.9578e-02,  5.8294e-02,\n",
       "           6.5964e-02,  3.2812e-03,  3.7626e-02,  9.6420e-02, -4.2926e-02,\n",
       "           5.4093e-02, -7.8043e-02, -9.1384e-02, -2.2752e-02, -2.3332e-02,\n",
       "          -2.6411e-02,  7.2564e-02, -8.4488e-03, -5.8523e-02, -5.0776e-02,\n",
       "           3.2122e-02, -9.7434e-02,  3.3793e-02,  2.0783e-02, -4.5941e-02,\n",
       "          -2.6204e-02, -9.7895e-02, -3.2598e-02,  5.6724e-02, -5.1443e-02,\n",
       "           2.9846e-02,  8.4554e-03, -4.5411e-02, -3.3259e-02, -2.0535e-02,\n",
       "          -4.3371e-02,  5.0148e-02,  4.6049e-02, -1.5824e-02, -7.2475e-02,\n",
       "           4.3322e-02, -3.9835e-02,  8.5319e-02, -2.0438e-02, -2.2051e-02,\n",
       "           1.0703e-02, -6.2848e-02, -3.4325e-02, -9.3368e-02,  9.2366e-02,\n",
       "          -8.3445e-02,  9.6797e-02, -9.0583e-02,  6.4380e-03,  2.7341e-02,\n",
       "           1.3602e-02, -7.7465e-03, -3.7057e-03, -6.5682e-02,  6.6202e-02,\n",
       "           6.0016e-02, -9.2970e-02,  8.4253e-02,  3.0414e-02, -8.5505e-02],\n",
       "         [-9.5348e-02, -7.6113e-02, -8.3606e-02,  1.5938e-02, -2.0131e-02,\n",
       "          -2.0699e-02,  3.4306e-04, -7.2682e-02, -6.7024e-03, -8.0483e-03,\n",
       "           9.7141e-03, -6.4989e-02, -6.9469e-02,  3.8024e-02,  5.0706e-02,\n",
       "           2.0047e-02, -2.3145e-02, -6.8218e-02, -5.4835e-02, -5.6266e-02,\n",
       "          -1.5731e-03,  5.5056e-03, -5.9776e-02, -8.9783e-02,  4.0810e-03,\n",
       "           1.0393e-02, -4.2363e-02,  4.7316e-02, -7.1609e-02, -3.1711e-02,\n",
       "           9.2264e-02, -8.2079e-02,  6.6912e-02,  6.4212e-02,  2.1305e-02,\n",
       "          -5.0689e-02, -1.9879e-02, -6.8521e-02,  5.3721e-02, -6.3786e-02,\n",
       "           7.4574e-02,  3.9781e-02, -1.5271e-02,  1.2718e-02, -3.4332e-02,\n",
       "           2.2917e-02,  9.0192e-02,  5.0204e-03,  9.2888e-02, -2.6175e-02,\n",
       "          -5.0326e-02,  9.9106e-02, -1.3483e-02, -7.1479e-02, -1.1978e-02,\n",
       "           4.7299e-02,  6.4482e-02, -5.9913e-02, -7.9387e-02,  9.2318e-02,\n",
       "           2.1284e-02,  9.9712e-02, -7.3051e-02,  9.0756e-02,  7.7301e-02,\n",
       "           9.8834e-02, -4.6585e-02, -6.4026e-02, -1.8898e-02,  6.0776e-02,\n",
       "          -2.7641e-02,  2.5494e-02,  7.2721e-02, -1.5766e-02,  4.7878e-02,\n",
       "          -4.2163e-02,  8.5962e-02,  7.3850e-02, -2.3346e-02, -2.6012e-02,\n",
       "           7.4358e-03,  9.6486e-02,  9.3332e-02, -6.5746e-02,  2.5491e-02,\n",
       "          -4.3182e-02, -5.5005e-02,  2.0531e-02, -1.5092e-02,  2.5460e-02,\n",
       "          -3.0333e-02, -8.8788e-02,  8.4583e-02, -1.8511e-03, -2.2102e-02,\n",
       "          -9.5434e-02, -1.9180e-02,  8.7092e-03,  5.7534e-02,  3.9332e-02],\n",
       "         [-4.6316e-02, -8.3404e-02, -7.9888e-02,  9.1124e-02,  6.2954e-02,\n",
       "           6.5575e-02, -9.3121e-02,  4.2876e-02,  8.9381e-02, -3.9886e-02,\n",
       "           4.9845e-02, -4.5368e-03,  7.6700e-02, -3.5977e-02,  5.4258e-02,\n",
       "           1.9079e-02,  3.3177e-02,  7.7526e-02,  8.3591e-02, -4.2260e-02,\n",
       "           6.7222e-02, -9.2025e-02,  8.3811e-02, -2.6344e-02,  3.6421e-02,\n",
       "           2.7206e-02, -1.9360e-02,  3.7738e-02, -5.1932e-02,  2.7489e-02,\n",
       "          -5.4084e-02, -5.5596e-02, -3.9697e-02, -3.2667e-02, -9.1111e-02,\n",
       "          -7.7667e-02,  5.7508e-02, -2.9396e-02,  7.9757e-02, -3.3773e-03,\n",
       "          -8.4354e-02, -9.1824e-02, -1.5001e-02, -3.5265e-03,  3.3553e-02,\n",
       "          -7.8544e-02,  7.0951e-02,  8.9415e-02, -6.8650e-02, -6.4837e-03,\n",
       "           4.1214e-02,  5.4215e-02,  1.6607e-02, -5.5795e-02, -2.3485e-02,\n",
       "          -5.2561e-02, -3.0251e-02,  8.7573e-02, -8.5913e-02, -5.2281e-02,\n",
       "          -3.9188e-02, -1.4096e-02,  6.3002e-02, -3.8720e-02,  6.5381e-02,\n",
       "          -1.7108e-04, -6.0910e-02,  8.9747e-02, -3.4480e-03,  3.5674e-02,\n",
       "          -9.3960e-02,  4.7458e-02, -9.9309e-02, -7.5367e-02,  6.8660e-02,\n",
       "           1.0661e-02, -1.1495e-02,  5.4564e-02,  2.3786e-03,  3.5486e-02,\n",
       "           7.3573e-02, -8.1635e-02, -6.5729e-02, -7.0058e-02, -2.7226e-02,\n",
       "          -6.3011e-02,  4.7394e-02, -5.4706e-02,  8.1106e-02,  2.3748e-03,\n",
       "          -3.7955e-02, -5.7144e-02, -6.7127e-02, -9.8912e-02,  6.3665e-02,\n",
       "           6.8999e-02,  6.7499e-02, -8.6455e-02,  1.0371e-02,  2.6472e-02]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0698, -0.0899,  0.0866, -0.0496,  0.0276, -0.0060, -0.0541,  0.0843,\n",
       "          0.0287, -0.0502], requires_grad=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p['params'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.0001\n",
      "momentum: 0\n",
      "nesterov: False\n",
      "weight_decay: 0\n"
     ]
    }
   ],
   "source": [
    "for optim_param in ('lr', 'momentum', 'nesterov', 'weight_decay'):\n",
    "    print(f'{optim_param}: {p[optim_param]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have to save a DL model, we definitely **need** to save model parameters (e.g. _inference_ ), but for other cases (i.e. _model checkpoint_ ) we **also need** to save **optimiser** `parameters` and `hyper-parameters`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `state_dict`\n",
    "\n",
    "A `state_dict` is an integral entity if you are interested in saving or loading models from PyTorch. \n",
    "\n",
    "Because `state_dict` objects are Python dictionaries, they can be easily saved, updated, altered, and restored, adding a great deal of modularity to PyTorch models and optimizers. \n",
    "\n",
    "Note that **only** layers with learnable parameters and registered buffers (e.g. batchnorm’s running_mean) have entries in the model’s `state_dict`. Optimizer objects (`torch.optim`) also have a `state_dict`, which contains information about the optimizer’s state, as well as the **hyperparameters** used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "linear1.weight \t torch.Size([100, 1000])\n",
      "linear1.bias \t torch.Size([100])\n",
      "linear2.weight \t torch.Size([10, 100])\n",
      "linear2.bias \t torch.Size([10])\n",
      "\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.0001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [4875991952, 4875992032, 4875992112, 4875992192]}]\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "print()\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading models for Inference in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instance of a `torch.nn.Module` can be saved using the `torch.save()` function.\n",
    "\n",
    "Saving the model’s `state_dict` with the `torch.save()` function will give you the most flexibility for restoring the model later. \n",
    "\n",
    "This is the **recommended method** for saving models, because it is only really necessary to save the trained model’s learned parameters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), \"model_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "model.load_state_dict(torch.load(\"model_state_dict.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common PyTorch **convention** is to save models using either a `.pt` or `.pth` file extension.\n",
    "\n",
    "Notice that the `load_state_dict()` function takes a dictionary object, NOT a `path` to a saved object. \n",
    "\n",
    "This means that you **must** deserialize the saved `state_dict` before you pass it to the `load_state_dict()` function. \n",
    "\n",
    "For example, you **CANNOT** just load using `model.load_state_dict(\"path_to_file.pt\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Saving and Loading Entire Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try the same thing with the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gu19087/opt/anaconda3/envs/dl-torch/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TwoLayerNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "torch.save(model, \"model.pth\")\n",
    "\n",
    "# Load\n",
    "model = torch.load(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight \t torch.Size([100, 1000])\n",
      "linear1.bias \t torch.Size([100])\n",
      "linear2.weight \t torch.Size([10, 100])\n",
      "linear2.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading model checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading a general `checkpoint model` for inference or resuming training can be helpful for picking up where you last left off. \n",
    "\n",
    "When saving a general checkpoint, you must save more than just the model’s `state_dict`. \n",
    "\n",
    "It is **also important** to save the **optimizer**’s `state_dict`, as this contains buffers and parameters that are updated as the model trains. \n",
    "\n",
    "**Moreover**, you might also want to save the `epoch` you left off on, the latest recorded `training loss`, external layers, and more, based on your own algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional information\n",
    "EPOCH = 5\n",
    "LOSS = 0.4\n",
    "CHKPOINT = \"model_checpoint.pth\"\n",
    "\n",
    "torch.save({'epoch': EPOCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, CHKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "checkpoint = torch.load(CHKPOINT)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "linear1.weight \t torch.Size([100, 1000])\n",
      "linear1.bias \t torch.Size([100])\n",
      "linear2.weight \t torch.Size([10, 100])\n",
      "linear2.bias \t torch.Size([10])\n",
      "\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.0001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [4898874624, 4898873904, 4896499024, 4898875584]}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "print()\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss from Checkpoint:  0.4\n",
      "Epoch:  5\n"
     ]
    }
   ],
   "source": [
    "print('Loss from Checkpoint: ', loss)\n",
    "print('Epoch: ', epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL-TORCH)",
   "language": "python",
   "name": "dl-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
