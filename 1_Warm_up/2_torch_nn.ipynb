{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch `nn` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Learning Process of a Neural Network\n",
    "\n",
    "![learning process sketch](./learning_process.png)\n",
    "\n",
    "<span class=\"fn\"><i>Source:</i> [1] - _Deep Learning with PyTorch_ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch `autograd`\n",
    "\n",
    "**TL;DR**:\n",
    "\n",
    "(doc. reference: [torch autograd](https://pytorch.org/docs/stable/autograd.html))\n",
    "\n",
    "`torch.autograd` provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. \n",
    "\n",
    "It requires minimal changes to the existing code - you only need to declare `Tensor`s for which gradients should be computed with the `requires_grad=True` keyword."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick look at the backend: Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TL; DR**: Automatic Differentiation lets you compute **exact** derivatives in **constant time**\n",
    "\n",
    "###### Teaser\n",
    "\n",
    "Automatic Differentiation is the secret sauce that powers most of the existing Deep Learning frameworks (e.g. Pytorch or TensorFlow). \n",
    "\n",
    "In a nutshell, Deep learnin frameworks provide the (technical) infrastructure in which computing the derivative of a function takes as much time as evaluating the function. In particular, the design idea is: \"you define a network with a loss function, and you get a gradient *for free*\".\n",
    "\n",
    "\n",
    "**Differentiation** in general is becoming a **first class citizen** in programming languages with early work started by Chris Lattner of LLVM famework — see the [Differentiable Programming Manifesto](https://github.com/apple/swift/blob/master/docs/DifferentiableProgramming.md) for more detail.\n",
    "\n",
    "However, if you're still wondering whether any of this is just boring math and how this relates to computer programming, I definitely suggest to have a look at **this** vide0 on YouTube:\n",
    "\n",
    "[GOTO 2018 - Machine Learning: Alchemy for the Modern Computer Scientist by Erik Meijer](https://www.youtube.com/watch?v=Rs0uRQJdIcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example\n",
    "\n",
    "Rather than talking about large neural networks, we will seek to understand automatic differentiation via a small problem borrowed from the book of *Griewank and Walther (2008)*.\n",
    "\n",
    "In the following we will adopt their very same **three-part** notation (also used in [4]).\n",
    "\n",
    "A function $f: \\mathbb{R^n} \\mapsto \\mathbb{R^m}$ is constructed using intermediate variables $v_i$ such that:\n",
    "\n",
    "- variables $v_{i-n} = x_i$, $i = 1,\\ldots,n$ are the input variables;\n",
    "- variables $v_i$, $i = 1,\\ldots,l$ are the working **intermediate** variables;\n",
    "- variables $y_{m-i} = v_{l-i}$, $i = m-1,\\ldots,0$ are the output variables.\n",
    "\n",
    "<img src=\"ad_example.png\" class=\"maxw80\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **traversal** of the graph and the **direction** in which gradients are actually computed defines the two modalities of AD:\n",
    "\n",
    "* **forward mode** AD;\n",
    "* **backward mode** AD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Forward (Tangent) Mode\n",
    "\n",
    "The idea of forward mode automatic differentiation is that we can compute derivatives as we go and\n",
    "that the chain rule says the overall derivative that we want is a composition of these incremental\n",
    "computations.\n",
    "\n",
    "Let’s imagine that our overall goal is to compute $\\frac{\\partial y}{\\partial x_1}$ for the example above.\n",
    "\n",
    "We denote all the intermediate *partial derivatives* (**with respect to $x_1$**) as:\n",
    "$$\n",
    "\\dot{v}_{i} = \\frac{\\partial v_i}{\\partial x_1}\n",
    "$$\n",
    "\n",
    "The **very same** applies when we want to compute $\\frac{\\partial y}{\\partial x_2}$\n",
    "\n",
    "<img src=\"forward_ad.png\" class=\"maxw80\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting bit is that we can implement this bookkeeping during the *execution trace* just via abstraction.\n",
    "\n",
    "We can replace our floating point numbers with `tuples`, and replace primitive functions with the following Python implementation (just using `numpy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add(atuple, btuple):\n",
    "    (a, adot) = atuple\n",
    "    (b, bdot) = btuple\n",
    "    return ( a + b, adot + bdot)\n",
    "\n",
    "def subtract(atuple, btuple): \n",
    "    (a, adot) = atuple\n",
    "    (b, bdot) = btuple\n",
    "    return (a - b, adot - bdot)\n",
    "\n",
    "def multiply(atuple, btuple):\n",
    "    (a, adot) = atuple\n",
    "    (b, bdot) = btuple\n",
    "    return (a * b, adot * b + bdot * a)\n",
    "\n",
    "def divide(atuple, btuple):\n",
    "    (a, adot) = atuple\n",
    "    (b, bdot) = btuple\n",
    "    return (a / b, (adot * b - bdot * a) / (b*b))\n",
    "\n",
    "def ln(atuple):\n",
    "    (a, adot) = atuple\n",
    "    return (np.log(a), (1/a)*adot)\n",
    "\n",
    "def sin(atuple):\n",
    "    (a, adot) = atuple\n",
    "    return (np.sin(a), np.cos(a)*adot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1: tuple, x2: tuple):\n",
    "    # ln(x1) + x1x2 - sin(x2)\n",
    "    v1 = ln(x1)\n",
    "    v2 = multiply(x1, x2)\n",
    "    v3 = add(v1, v2)\n",
    "    v4 = sin(x2)\n",
    "    v5 = subtract(v3, v4)\n",
    "    return v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.652071455223084, 5.5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x1=(2, 1), x2=(5, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.652071455223084, 1.7163378145367738)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x1=(2, 0), x2=(5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Reverse (Co-Tangent) Mode\n",
    "\n",
    "AD in the reverse accumulation mode corresponds to a generalized backpropagation algorithm, in that it propagates derivatives backward from a given output. This is done by complementing each intermediate variable $v_i$ with an **adjoint**:\n",
    "$$\n",
    "\\bar{v}_{i} = \\frac{\\partial y_i}{\\partial v_i} = \\displaystyle{\\sum_{j:\\text{child of i}} \\bar{vj} \\frac{\\partial v_j}{\\partial v_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"backward_ad.png\" class=\"maxw85\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/google/tangent/raw/master/docs/toolspace.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways to implement this abstraction in its full generality, but an implementation requires more code than can easily appear here. The three major approaches are:\n",
    "\n",
    "**source code transformation**: The adjoint backward pass code is generated a priori from the forward computation. A clean Python example of such a system is [**Tangent**](https://colab.research.google.com/drive/1cjoX9GteBymbnqcikNMZP1uenMcwAGDe).\n",
    "\n",
    "**graph-based**: This approach uses an embedded mini-language to specify a graph of computations that can then be manipulated for function evaluations and gradients. \n",
    "\n",
    "$\\rightarrow$ The advantage of this approach is that it is amenable to intelligent graph optimizations and use of compilers. The embedded mini-language also makes it possible to build specialized hardware that targets the differentiable primitives. \n",
    "\n",
    "$\\rightarrow$ The downside of this approach is that you are not coding in the host language (e.g., Python) and so you can’t take advantage of its imperative design and control flow. Generally the mini-language is less expressive than the host language. Also, the lazy execution of the function represented by the graph can make it difficult to debug. TensorFlow 1.x is an example of this kind of automatic differentiation.\n",
    "\n",
    "**tape-based**: This approach tracks the actual composed functions as they are called during execution of the forward pass. One name for this data structure is the *Wengert list*. \n",
    "\n",
    "$\\rightarrow$ With the ordered sequence of computations in hand, it is then possible to walk backward through the list to compute the gradient. \n",
    "\n",
    "$\\rightarrow$ The advantage of this is that it can more easily use all the features of the host language and the imperative execution is easier to understand. \n",
    "\n",
    "$\\rightarrow$ The downside is that it can be more difficult to optimize the code and reuse computations across executions. \n",
    "\n",
    "[Autograd](https://github.com/HIPS/autograd) is an example of this. \n",
    "The automatic differentiation in [PyTorch](https://pytorch.org/) also roughly follows this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### References and Futher Reading\n",
    "\n",
    "1. [Deep Learning with PyTorch (**free sample**) - Luca Antiga et. al.](https://pytorch.org/deep-learning-with-pytorch)\n",
    "\n",
    "4. [(*Paper*) Automatic Differentiation in Machine Learning: a Survey](https://arxiv.org/abs/1502.05767)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.nn` in a Nutshell\n",
    "\n",
    "Computational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives; however for large neural networks raw autograd can be a bit too low-level.\n",
    "\n",
    "When building neural networks we frequently think of arranging the computation into layers, some of which \n",
    "have learnable parameters which will be optimized during learning.\n",
    "\n",
    "In TensorFlow, packages like **Keras**, (old **TensorFlow-Slim**, and **TFLearn**) provide higher-level abstractions over raw computational graphs that are useful for building neural networks.\n",
    "\n",
    "In PyTorch, the `nn` package serves this same purpose. \n",
    "\n",
    "The `nn` package defines a set of `Module`s, which are roughly equivalent to neural network layers. \n",
    "\n",
    "A `Module` receives input `Tensor`s and computes output `Tensor`s, but may also hold internal state such as `Tensor`s containing learnable parameters. \n",
    "\n",
    "The `nn` package also defines a set of useful `loss` functions that are commonly used when \n",
    "training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PyTorch Examples\n",
    "\n",
    "The following examples have been extracted from the [PyTorch Examples Repository](https://github.com/jcjohnson/pytorch-examples) by `@jcjohnson`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we use the `nn` package to implement our two-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 642.0931396484375\n",
      "50 39.40785217285156\n",
      "100 2.866370439529419\n",
      "150 0.3431205451488495\n",
      "200 0.05561559647321701\n",
      "250 0.010967700742185116\n",
      "300 0.002458262722939253\n",
      "350 0.0005952278152108192\n",
      "400 0.00015158375026658177\n",
      "450 3.962670962209813e-05\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 50 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.optim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point we have updated the weights of our models by manually mutating the Tensors holding learnable parameters (**using `torch.no_grad()` or `.data` to avoid tracking history in autograd**). \n",
    "\n",
    "This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like `AdaGrad`, `RMSProp`, \n",
    "`Adam`.\n",
    "\n",
    "The optim package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms.\n",
    "\n",
    "Let's finally modify the previous example in order to use `torch.optim` and the `Adam` algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model and Optimiser (w/ Parameters) at a glance\n",
    "\n",
    "![model_and_optimiser](model_optim.png)\n",
    "\n",
    "<span class=\"fn\"><i>Source:</i> [1] - _Deep Learning with PyTorch_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 702.6163940429688\n",
      "50 219.6895751953125\n",
      "100 61.97289276123047\n",
      "150 11.652957916259766\n",
      "200 1.4025999307632446\n",
      "250 0.1275288462638855\n",
      "300 0.009245814755558968\n",
      "350 0.0005796181503683329\n",
      "400 3.2619711419101804e-05\n",
      "450 1.6339765807060758e-06\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 50 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we do better ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible scenario:\n",
    "\n",
    "- Specify models that are more complex than a sequence of existing (pre-defined) modules;\n",
    "- Customise the learning procedure (e.g. _weight sharing_ ?)\n",
    "- ?\n",
    "\n",
    "For these cases, **PyTorch** allows to define our own custom modules by subclassing `nn.Module` and defining a `forward` method which receives the input data (i.e. `Tensor`) and returns the output (i.e. `Tensor`).\n",
    "\n",
    "It is in the `forward` method that **all** the _magic_ of Dynamic Graph and `autograd` operations happen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Custom Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's implement our **two-layers** model as a custom `nn.Module` subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.hidden_activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        l1 = self.linear1(x)\n",
    "        h_relu = self.hidden_activation(l1)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 716.993896484375\n",
      "50 28.708864212036133\n",
      "100 1.3810720443725586\n",
      "150 0.1258920580148697\n",
      "200 0.016454389318823814\n",
      "250 0.0026553189381957054\n",
      "300 0.0004969889996573329\n",
      "350 0.00010043554357253015\n",
      "400 2.1219193513388745e-05\n",
      "450 4.619778337655589e-06\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 50 == 0:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happened really? Let's have a closer look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> model = TwoLayerNet(D_in, H, D_out)\n",
    "```\n",
    "\n",
    "This calls `TwoLayerNet.__init__` **constructor** method (_implementation reported below_ ):\n",
    "\n",
    "```python\n",
    "def __init__(self, D_in, H, D_out):\n",
    "    \"\"\"\n",
    "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "    member variables.\n",
    "    \"\"\"\n",
    "    super(TwoLayerNet, self).__init__()\n",
    "    self.linear1 = torch.nn.Linear(D_in, H)\n",
    "    self.hidden_activation = torch.nn.ReLU()\n",
    "    self.linear2 = torch.nn.Linear(H, D_out)\n",
    "```\n",
    "\n",
    "1. First thing, we call the `nn.Module` constructor which sets up the housekeeping\n",
    "    - If you forget to do that, you will get and error message reminding that you should call it before using any `nn.Module` capabilities\n",
    "2. We create a class attribute for each layer (`OP/Tensor/`) that we intend to include in our model\n",
    "    - These can be also `Sequential` as in _Submodules_ or *Block of Layers*\n",
    "    - **Note**: We are **not** defining the Graph yet, just the layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> y_pred = model(x)\n",
    "```\n",
    "\n",
    "1. First thing to notice: the `model` object is **callable**\n",
    "   - It means `nn.Module` is implementing a `__call__` method\n",
    "   - We **don't** need to re-implement that!\n",
    "   \n",
    "2. (in fact) The `nn.Module` class will call `self.forward` - in a [Template Method Pattern](https://en.wikipedia.org/wiki/Template_method_pattern) fashion\n",
    "    - for this reason, we have to define the `forward` method\n",
    "    - (needless to say) the `forward` method implements the **forward** pass of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from torch.nn.modules.module.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "class Module(object):\n",
    "    # [...] omissis\n",
    "    def __call__(self, *input, **kwargs):\n",
    "        for hook in self._forward_pre_hooks.values():\n",
    "            result = hook(self, input)\n",
    "            if result is not None:\n",
    "                if not isinstance(result, tuple):\n",
    "                    result = (result,)\n",
    "                input = result\n",
    "        if torch._C._get_tracing_state():\n",
    "            result = self._slow_forward(*input, **kwargs)\n",
    "        else:\n",
    "            result = self.forward(*input, **kwargs)\n",
    "        for hook in self._forward_hooks.values():\n",
    "            hook_result = hook(self, input, result)\n",
    "            if hook_result is not None:\n",
    "                result = hook_result\n",
    "        if len(self._backward_hooks) > 0:\n",
    "            var = result\n",
    "            while not isinstance(var, torch.Tensor):\n",
    "                if isinstance(var, dict):\n",
    "                    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))\n",
    "                else:\n",
    "                    var = var[0]\n",
    "            grad_fn = var.grad_fn\n",
    "            if grad_fn is not None:\n",
    "                for hook in self._backward_hooks.values():\n",
    "                    wrapper = functools.partial(hook, self)\n",
    "                    functools.update_wrapper(wrapper, hook)\n",
    "                    grad_fn.register_hook(wrapper)\n",
    "        return result\n",
    "    \n",
    "    # [...] omissis\n",
    "    def forward(self, *input):\n",
    "        r\"\"\"Defines the computation performed at every call.\n",
    "\n",
    "        Should be overridden by all subclasses.\n",
    "\n",
    "        .. note::\n",
    "            Although the recipe for forward pass needs to be defined within\n",
    "            this function, one should call the :class:`Module` instance afterwards\n",
    "            instead of this since the former takes care of running the\n",
    "            registered hooks while the latter silently ignores them.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take away messages** :\n",
    "1. We don't need to implement the `__call__` method at all in our custom model subclass\n",
    "2. We don't need to call the `forward` method directly. \n",
    "    - We could, but we would miss the flexibility of _forward_ and _backwar_ hooks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Last but not least\n",
    "\n",
    "```python\n",
    ">>> optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "```\n",
    "\n",
    "Being `model` a subclass of `nn.Module`, `model.parameters()` will automatically capture all the `Layers/OP/Tensors/Parameters` that require gradient computation, so to feed to the `autograd` engine during the *backward* (optimisation) step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### `model.named_parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight        torch.Size([100, 1000]) 100000\n",
      "linear1.bias          torch.Size([100])   100\n",
      "linear2.weight        torch.Size([10, 100]) 1000\n",
      "linear2.bias          torch.Size([10])    10\n"
     ]
    }
   ],
   "source": [
    "for name_str, param in model.named_parameters():\n",
    "    print(\"{:21} {:19} {}\".format(name_str, str(param.shape), param.numel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WAIT**: What happened to `hidden_activation` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "self.hidden_activation = torch.nn.ReLU()\n",
    "```\n",
    "\n",
    "So, it looks that we are registering in the constructor a submodule (`torch.nn.ReLU`) that has no parameters.\n",
    "\n",
    "Generalising, if we would've had **more** (hidden) layers, it would have required the definition of one of these submodules for each pair of layers (at least)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking back at the implementation of the `TwoLayerNet` class as a whole, it looks like a bit of a waste.\n",
    "\n",
    "**Can we do any better here?** 🤔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, in this particular case, we could implement the `ReLU` activation _manually_, it is not that difficult, isn't it?\n",
    "\n",
    "$\\rightarrow$ As we already did before, we could use the [`torch.clamp`](https://pytorch.org/docs/stable/torch.html?highlight=clamp#torch.clamp) function\n",
    "\n",
    "> `torch.clamp`: Clamp all elements in input into the range [ min, max ] and return a resulting tensor\n",
    "\n",
    "`t.clamp(min=0)` is **exactly** the ReLU that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sorted!\n",
    "\n",
    "That was easy, wasn't it? **However**, what if we wanted *other* activation functions (e.g. `tanh`, \n",
    "`sigmoid`, `LeakyReLU`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introducing the Functional API\n",
    "\n",
    "PyTorch has functional counterparts of every `nn` module. \n",
    "\n",
    "By _functional_ here we mean \"having no internal state\", or, in other words, \"whose output value is solely and fully determined by the value input arguments\". \n",
    "\n",
    "Indeed, `torch.nn.functional` provides the many of the same modules we find in `nn`, but with all eventual parameters moved as an argument to the function call. \n",
    "\n",
    "For instance, the functional counterpart of `nn.Linear` is `nn.functional.linear`, which is a function that has signature `linear(input, weight, bias=None)`. \n",
    "\n",
    "The `weight` and `bias` parameters are **arguments** to the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our `TwoLayerNet` model, it makes sense to keep using nn modules for `nn.Linear`, so that our model will be able to manage all of its `Parameter` instances during training. \n",
    "\n",
    "However, we can safely switch to the functional counterparts of `nn.ReLU`, since it has no parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = torch.nn.functional.relu(self.linear1(x))  # torch.relu would do as well\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 698.4146728515625\n",
      "50 34.69792556762695\n",
      "100 2.164041042327881\n",
      "150 0.23706716299057007\n",
      "200 0.03495536744594574\n",
      "250 0.006048018112778664\n",
      "300 0.001142494729720056\n",
      "350 0.00022856226132716984\n",
      "400 4.769738734466955e-05\n",
      "450 1.0313570783182513e-05\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 50 == 0:\n",
    "        print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightarrow$ For the curious minds: [The difference and connection between torch.nn and torch.nn.function from relu's various implementations](https://programmer.group/5d5a404b257d7.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Latest from the `torch` ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\rightarrow$: [Migration from Chainer to PyTorch](https://medium.com/pytorch/migration-from-chainer-to-pytorch-8ed92c12c8)\n",
    "\n",
    "* $\\rightarrow$: [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/introduction_guide.html)\n",
    "    - [fast.ai](https://docs.fast.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Collaborative)\n",
    "\n",
    "_Implementing the Perceptron_ in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/rasbt/python-machine-learning-book-3rd-edition/master/ch02/images/02_09.png\" />\n",
    "\n",
    "Perceptron (Adaline algorithm) - **ADAptive LInear Neuron** main features:\n",
    "\n",
    "- Unit Step function\n",
    "- Binary Classification \n",
    "- Linear Model\n",
    "- Weight Update rule with `learning rate` and Gradient Descent Optimisation\n",
    "\n",
    "$\n",
    "    {\\bf w} = {\\bf w} + \\Delta {\\bf w} \\text{, where } \\Delta {\\bf w} = -\\eta \\nabla J({\\bf w}) \n",
    "$ [$^2$](#fn2)\n",
    "\n",
    "<span id=\"fn2\">2: ${\\bf w}$ (in bold) refers to the **weights vector**. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-05aa2a3b7b29>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-05aa2a3b7b29>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    super(Perceptron, self).__init__() self.fc1 = nn.Linear(input_dim, 1)\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Perceptron(nn.Module):\n",
    "    \"\"\" A perceptron is one linear layer \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\" Args:\n",
    "        input_dim (int): size of the input features\n",
    "        \"\"\"\n",
    "        super(Perceptron, self).__init__() self.fc1 = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, x_in):\n",
    "        \"\"\"The forward pass of the perceptron\n",
    "        Args:\n",
    "        x_in (torch.Tensor): an input data tensor\n",
    "        x_in.shape should be (batch, num_features) Returns:\n",
    "        the resulting tensor. tensor.shape should be (batch,).\n",
    "        \"\"\"\n",
    "        return torch.sigmoid(self.fc1(x_in)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A review of the main `activation` functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sigmoid\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "x = torch.arange(-5., 5., 0.1) \n",
    "\n",
    "y = torch.sigmoid(x) \n",
    "\n",
    "plt.plot(x.numpy(), y.numpy()) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tanh\n",
    "\n",
    "$$\n",
    "f(x) = tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "x = torch.arange(-5., 5., 0.1) \n",
    "\n",
    "y = torch.tanh(x) \n",
    "\n",
    "plt.plot(x.numpy(), y.numpy()) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "x = torch.arange(-5., 5., 0.1) \n",
    "\n",
    "y = torch.nn.functional.relu(x) \n",
    "\n",
    "plt.plot(x.numpy(), y.numpy()) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\max(x, \\alpha x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prelu = torch.nn.PReLU(num_parameters=1)\n",
    "x = torch.arange(-5., 5., 0.1) \n",
    "\n",
    "y = prelu(x) \n",
    "\n",
    "plt.plot(x.detach().numpy(), y.detach().numpy()) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch\n",
    "\n",
    "softmax = nn.Softmax(dim=1) \n",
    "\n",
    "x_input = torch.randn(1, 3) \n",
    "y_output = softmax(x_input) \n",
    "\n",
    "print(x_input)\n",
    "print(y_output) \n",
    "\n",
    "print(torch.sum(y_output, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loss Functions\n",
    "\n",
    "```python\n",
    "CrossEntropyLoss == LogSoftmax + NLLLoss\n",
    "BCEWithLogits == LogSigmoid + NLLLoss\n",
    "MSELoss(reduce=sum) == SSE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### References and Futher Reading:\n",
    "\n",
    "1. [Deep Learning with PyTorch, Luca Antiga et. al.](https://www.manning.com/books/deep-learning-with-pytorch)\n",
    "2. [(**Terrific**) PyTorch Examples Repo](https://github.com/jcjohnson/pytorch-examples) (*where most of the examples in this notebook have been adapted from*)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL-TORCH)",
   "language": "python",
   "name": "dl-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
